{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Dependencies\n",
    "For this tutorial we will be needing the following depencies, we expreminted in the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install tqdm\n",
    "!pip install onnxruntime-gpu\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torchvision \n",
    "from timeit import timeit\n",
    "\n",
    "N = 100\n",
    "example = torch.rand(1, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ordinary Torch Model\n",
    "We begin by downloading the pre-trained ResNet50 pytorch model from the torchhub, then we setup the timing function, and last, we always add the model. cuda() was used since the data must transit from the ram to the gpu's vram while running infernce on cuda, and we wanted to compare it fairly to onnx runtime that uses a numpy Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Pytorch Resnet50\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "def inf_torch(example):\n",
    "  with torch.no_grad():\n",
    "    output = model(example.cuda())\n",
    "    torch.cuda.synchronize()\n",
    "    return output\n",
    "example = example.cpu()\n",
    "\n",
    "torch_t = timeit(lambda : inf_torch(example), number=N)/N\n",
    "torch_output = F.softmax(inf_torch(example), dim=1).topk(1).indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scripting the model \n",
    "Convert the model to an IR using scripting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TorchScript\n",
    "traced_script_module = torch.jit.trace(model, example.cuda())\n",
    "traced_script_module.eval()\n",
    "traced_script_module.cuda()\n",
    "\n",
    "def inf_torch_scripted(example):\n",
    "  with torch.no_grad():\n",
    "    output = traced_script_module(example.cuda())\n",
    "    torch.cuda.synchronize()\n",
    "    return output\n",
    "\n",
    "example = example.cpu()\n",
    "# We count the time it takes to pas the data from cpu to gpu \n",
    "scripted_t = timeit(lambda : inf_torch_scripted(example), number=N)/N\n",
    "scripted_output = F.softmax(inf_torch_scripted(example), dim=1).topk(1).indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert To Onnx\n",
    "We convert the pytorch model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.onnx import TrainingMode\n",
    "\n",
    "# Export the model\n",
    "def convert_to_onnx(\n",
    "    model_pytorch , output_path: str, inputs_pytorch, opset: int = 12\n",
    ") -> None:\n",
    "    # dynamic axis == variable length axis\n",
    "    dynamic_axis = OrderedDict()\n",
    "    for k in inputs_pytorch.keys():\n",
    "        dynamic_axis[k] = { 0: \"batch_size\"}\n",
    "    dynamic_axis[\"output\"] = { 0: \"batch_size\"}\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model_pytorch,  # model to optimize\n",
    "            args=tuple(inputs_pytorch.values()),  # tuple of multiple inputs\n",
    "            f=output_path,  # output path / file object\n",
    "            opset_version=opset,  # the ONNX version to use, 13 if quantized model, 12 for not quantized ones\n",
    "            do_constant_folding=True,  # simplify model (replace constant expressions)\n",
    "            input_names=list(inputs_pytorch.keys()),  # input names\n",
    "            output_names=[\"output\"],  # output axis name\n",
    "            dynamic_axes=dynamic_axis,  # declare dynamix axis for each input / output\n",
    "            training=TrainingMode.EVAL,  # always put the model in evaluation mode\n",
    "            verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create this folder structure for Triton in the next tutorial\n",
    "\n",
    "!mkdir -p models/onnx-model-gpu\n",
    "!mkdir -p models/onnx-model-gpu/\n",
    "\n",
    "convert_to_onnx(model.cuda(),'models/onnx-model-gpu/1/model.onnx',{\"input\":example.cuda()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have model exported as onnx, we will be using onnx-runtime to run the onnx with different runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the onnx with Cuda Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = example.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_model = onnxruntime.InferenceSession('models/onnx-model-gpu/1/model.onnx',\n",
    "providers=[\n",
    "    ('CUDAExecutionProvider',\n",
    "    {\n",
    "        'device_id': 0,\n",
    "    })\n",
    "])\n",
    "\n",
    "def onnx_inf(data):\n",
    "    return onnx_model.run(None,{\n",
    "                onnx_model.get_inputs()[0].name: data\n",
    "           })\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warmup\n",
    "onnx_inf(data)\n",
    "onnx_t = timeit(lambda : onnx_inf(data), number=N)/N\n",
    "onnx_output = np.argpartition(onnx_inf(data)[0][0],-1)[-1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model with TensorRT Backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_model = onnxruntime.InferenceSession('models/onnx-model-gpu/1/model.onnx',\n",
    "providers=[\n",
    "    ('TensorrtExecutionProvider',\n",
    "    {\n",
    "        'device_id': 0,\n",
    "    })\n",
    "])\n",
    "\n",
    "data = example.cpu().detach().numpy()\n",
    "\n",
    "def tensorrt_inf(data):\n",
    "    return onnx_model.run(None,{\n",
    "                onnx_model.get_inputs()[0].name: data\n",
    "           })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 00:46:16.180290989 [W:onnxruntime:Default, tensorrt_execution_provider.h:53 log] [2022-03-04 00:46:16 WARNING] Detected invalid timing cache, setup a local cache instead\n"
     ]
    }
   ],
   "source": [
    "#warmup\n",
    "tensorrt_inf(data)\n",
    "tensorrt_inf_t = timeit(lambda : tensorrt_inf(data), number=N)/N\n",
    "tensorrt_inf_output  = np.argpartition(tensorrt_inf(data)[0][0],-1)[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run the model with TensorRT FP16 Backend\n",
    "In this example we ran the same TensoRT model, but in FP16 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_model = onnxruntime.InferenceSession('models/onnx-model-gpu/1/model.onnx',\n",
    "providers=[\n",
    "    ('TensorrtExecutionProvider',\n",
    "    {\n",
    "        'device_id': 0,\n",
    "        'trt_fp16_enable':True,\n",
    "    })\n",
    "])\n",
    "\n",
    "data = example.cpu().detach().numpy()\n",
    "\n",
    "def tensorrt_inf_fp16(data):\n",
    "    return onnx_model.run(None,{\n",
    "                onnx_model.get_inputs()[0].name: data\n",
    "           })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 00:46:29.282300304 [W:onnxruntime:Default, tensorrt_execution_provider.h:53 log] [2022-03-04 00:46:29 WARNING] Detected invalid timing cache, setup a local cache instead\n"
     ]
    }
   ],
   "source": [
    "#warmup phase \n",
    "timeit(lambda : tensorrt_inf_fp16(data), number=N)/N\n",
    "\n",
    "tensorrt_inf_fp16_t = timeit(lambda : tensorrt_inf_fp16(data), number=N)/N\n",
    "tensorrt_inf_fp16_t_output = np.argpartition(tensorrt_inf_fp16(data)[0][0],-1)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results compraion : \n",
      "         PyTorch-cuda 0.005198871344327926 \n",
      "         Pytorch-cuda-scripted 0.00447928112000227 \n",
      "         onnx-cuda 0.0031806880980730055 \n",
      "         TensorRT 0.0025665512308478355 \n",
      "         TensorRT-FP16 0.0011617697402834892\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results compraion : \\n \\\n",
    "        PyTorch-cuda {torch_t} \\n \\\n",
    "        Pytorch-cuda-scripted {scripted_t} \\n \\\n",
    "        onnx-cuda {onnx_t} \\n \\\n",
    "        TensorRT {tensorrt_inf_t} \\n \\\n",
    "        TensorRT-FP16 {tensorrt_inf_fp16_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeedUPs : \n",
      "         PyTorch-cuda 1.0 \n",
      "         Pytorch-cuda-scripted 1.1606485962920075 \n",
      "         onnx-cuda 1.6345115220438058 \n",
      "         TensorRT 2.0256253924884753 \n",
      "         TensorRT-FP16 4.474958474180369\n"
     ]
    }
   ],
   "source": [
    "print(f\"SpeedUPs : \\n \\\n",
    "        PyTorch-cuda {torch_t/torch_t} \\n \\\n",
    "        Pytorch-cuda-scripted {torch_t/scripted_t} \\n \\\n",
    "        onnx-cuda {torch_t/onnx_t} \\n \\\n",
    "        TensorRT {torch_t/tensorrt_inf_t} \\n \\\n",
    "        TensorRT-FP16 {torch_t/tensorrt_inf_fp16_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization INT8 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized = torchvision.models.quantization.resnet50(pretrained=True, quantize=True)\n",
    "model_quantized.eval()\n",
    "\n",
    "def inf_quantized(example):\n",
    "  with torch.no_grad():\n",
    "    output = model_quantized(example)\n",
    "    torch.cuda.synchronize()\n",
    "    return output\n",
    "example = example.cpu()\n",
    "\n",
    "quantized_inf_t = timeit(lambda : inf_quantized(example), number=N)/N\n",
    "quantized_inf_output = F.softmax(inf_quantized(example), dim=1).topk(1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "def inf_torch_cpu(example):\n",
    "  with torch.no_grad():\n",
    "    output = model(example)\n",
    "    torch.cuda.synchronize()\n",
    "    return output\n",
    "example = example.cpu()\n",
    "\n",
    "torch_cpu_t = timeit(lambda : inf_torch_cpu(example), number=N)/N\n",
    "torch_cpu_output = F.softmax(inf_torch_cpu(example), dim=1).topk(1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeedUPs : \n",
      "         PyTorch-CPU 1.0 \n",
      "         INT8-CPU 1.9307451741422934\n"
     ]
    }
   ],
   "source": [
    "print(f\"SpeedUPs : \\n \\\n",
    "        PyTorch-CPU {torch_cpu_t/torch_cpu_t} \\n \\\n",
    "        INT8-CPU {torch_cpu_t/quantized_inf_t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Comparing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs : \n",
      "         PyTorch-cuda tensor([[610]], device='cuda:0') \n",
      "         Pytorch-cuda-scripted tensor([[610]], device='cuda:0') \n",
      "         onnx-cuda [610] \n",
      "         TensorRT [610] \n",
      "         TensorRT-FP16 [610] \n",
      "         INT8 tensor([[610]])       \n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs : \\n \\\n",
    "        PyTorch-cuda {torch_output} \\n \\\n",
    "        Pytorch-cuda-scripted {scripted_output} \\n \\\n",
    "        onnx-cuda {onnx_output} \\n \\\n",
    "        TensorRT {tensorrt_inf_output} \\n \\\n",
    "        TensorRT-FP16 {tensorrt_inf_fp16_t_output} \\n \\\n",
    "        INT8 {quantized_inf_output}       \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), 'temp_saved_model.pt')\n",
    "    model_size_in_mb = os.path.getsize('temp_saved_model.pt') >> 20\n",
    "    os.remove('temp_saved_model.pt')\n",
    "    return model_size_in_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size : \n",
      "         original model size 97 mb \n",
      "         quantized_model_size 24 mb\n"
     ]
    }
   ],
   "source": [
    "original_model_size = get_model_size(model)\n",
    "quantized_model_size = get_model_size(model_quantized)\n",
    "\n",
    "print(f\"Model size : \\n \\\n",
    "        original model size {original_model_size} mb \\n \\\n",
    "        quantized_model_size {quantized_model_size} mb\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
